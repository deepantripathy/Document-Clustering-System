{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [SSL:\n",
      "[nltk_data]     CERTIFICATE_VERIFY_FAILED] certificate verify failed:\n",
      "[nltk_data]     unable to get local issuer certificate (_ssl.c:1007)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from pdfminer.high_level import extract_text\n",
    "import pickle\n",
    "from rake_nltk import Rake\n",
    "import nltk\n",
    "import shutil\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF texts extracted: 56\n"
     ]
    }
   ],
   "source": [
    "num_clusters = 3\n",
    "\n",
    "pdf_folder = 'Final_Dataset/'\n",
    "pdf_files = [os.path.join(pdf_folder, f) for f in os.listdir(pdf_folder) if f.endswith(\".pdf\")]\n",
    "pdf_texts = []\n",
    "pdf_labels = []\n",
    "for pdf_file in pdf_files:\n",
    "    pdf_text = extract_text(pdf_file)\n",
    "    if pdf_text is not None:\n",
    "        pdf_texts.append(pdf_text)\n",
    "        pdf_labels.append(os.path.basename(pdf_file))\n",
    "\n",
    "if pdf_texts:\n",
    "    print(f\"PDF texts extracted: {len(pdf_texts)}\")\n",
    "else:\n",
    "    print(\"PDF error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and preprocess text\n",
    "tokenized_docs = [gensim.utils.simple_preprocess(text) for text in pdf_texts]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc2vec model created; word2vec_model.pkl created\n"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model\n",
    "word2vec_model = Word2Vec(tokenized_docs, vector_size=300, window=10, min_count=2, workers=4, sg=1) #epochs=100)\n",
    "with open('Models/word2vec_model.pkl', 'wb') as f:\n",
    "    pickle.dump(word2vec_model, f)\n",
    "    print(\"Doc2vec model created; word2vec_model.pkl created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kmeans Doc2vec model creation successful; kmeans_model_word2vec.pkl created\n"
     ]
    }
   ],
   "source": [
    "# Cluster the documents using K-means clustering\n",
    "# doc_vectors = [np.mean([word2vec_model.wv[token] for token in doc], axis=0) for doc in tokenized_docs]\n",
    "keys = list(word2vec_model.wv.key_to_index.keys())\n",
    "doc_vectors = []\n",
    "for doc in tokenized_docs:\n",
    "    vec = np.zeros(word2vec_model.vector_size)\n",
    "    count = 0\n",
    "    for word in doc:\n",
    "        if word in keys:\n",
    "            vec += word2vec_model.wv[word]\n",
    "            count += 1\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    doc_vectors.append(vec)\n",
    "\n",
    "kmeans_model = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100, n_init=1)\n",
    "kmeans_model.fit(doc_vectors)\n",
    "with open('Models/kmeans_model_word2vec.pkl', 'wb') as f:\n",
    "    pickle.dump(kmeans_model, f)\n",
    "    print(\"Kmeans Doc2vec model creation successful; kmeans_model_word2vec.pkl created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category Labels: [1 0 0 0 1 0 1 0 0 0 1 0 0 1 0 1 0 0 1 1 1 0 1 1 1 0 0 0 1 1 1 0 0 0 0 1 1\n",
      " 1 1 0 1 1 0 0 1 1 0 0 1 0 0 1 1 0 1 0]\n"
     ]
    }
   ],
   "source": [
    "# Identify the category using centroid analysis\n",
    "centroids = kmeans_model.cluster_centers_\n",
    "similarity_matrix = cosine_similarity(doc_vectors, centroids)\n",
    "category_labels = np.argmax(similarity_matrix, axis=1)\n",
    "print(f\"Category Labels: {category_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the document to the corresponding category\n",
    "result_df = pd.DataFrame({'Document': pdf_files, 'Category': category_labels})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract keywords from documents using RAKE\n",
    "r = Rake(stopwords=nltk.corpus.stopwords.words('english'), min_length=1, max_length=3)\n",
    "keywords = []\n",
    "for text in pdf_texts:\n",
    "    r.extract_keywords_from_text(text)\n",
    "    keywords.append(r.get_ranked_phrases())\n",
    "    \n",
    "# Map the category labels to keywords\n",
    "category_keywords = [keywords[label][0] for label in category_labels]\n",
    "result_df['Category Keyword'] = category_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_path = \"Final_Results/Bag_Words_Approach/\"\n",
    "# Create directories with names from category_keyword\n",
    "for idx, row in result_df.iterrows():\n",
    "    category_keyword = row['Category Keyword']\n",
    "    folder_name = os.path.join(result_path, category_keyword)\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Copy the corresponding document in that folder\n",
    "for idx, row in result_df.iterrows():\n",
    "    source_file = row['Document']\n",
    "    category_keyword = row['Category Keyword']\n",
    "    folder_name = os.path.join(result_path, category_keyword)\n",
    "    shutil.copy(source_file, folder_name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
